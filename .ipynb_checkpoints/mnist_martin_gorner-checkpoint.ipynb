{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "* Dataset used - MNIST (with one-hot-encoding)\n",
    "* LOG_DIR = \"tmp/\"\n",
    "* PLOT_DIR = './out/plots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import utility\n",
    "\n",
    "# import DataSet\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True, reshape=False, validation_size=0)\n",
    "\n",
    "LOG_DIR = \"tmp/\"\n",
    "PLOT_DIR = './out/plots'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input and Ground-truth placeholders  \n",
    "* One image is a 28x28 images with 0's and 1's  \n",
    "* Ground-truth is a label from 0-9  \n",
    ".  \n",
    "* Y  = ground-truth  \n",
    "* Y-underscore = predicted-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and train_step\n",
    "* learning_rate\n",
    "* number_of_epochs\n",
    "* batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input 28-by-28 pixels images of GRAYSCALE\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1], name='X')\n",
    "# Output in 'one-hot-encoding', 10 classes\n",
    "Y_ = tf.placeholder(tf.float32, [None, 10], name='Y_')\n",
    "# placeholders for hyper parameters\n",
    "keep_prob = tf.placeholder(tf.float32, [], name='dropout_probability')\n",
    "learning_rate = tf.placeholder(tf.float32, [], name='learning_rate')\n",
    "n_epochs = 2000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "* **Input-layer**[28x28x1 images] >> (W1, b1) >> **conv-layer-1**[28x28x4 maps]\n",
    "* **conv-layer-1**[28x28x4 maps] >> (W2, b2) >> **conv-layer-2**[14x14x8 maps]\n",
    "* **conv-layer-2**[14x14x8 maps] >> (W3, b3) >> **conv-layer-3**[7x7x12 maps]\n",
    "* **conv-layer-3**[7x7x12 maps] >> (W4, b4) >> **fcc-layer-**[200 neurons]\n",
    "* **fcc-layer**[200 neurons] >> (W5, b5) >> **Output-layer**[10 neurons]  \n",
    ".  \n",
    "* **sigmoid/relu** activation function used in hidden layers   \n",
    "* **softmax** activation function used at output layer   \n",
    " \n",
    "### Cost function  \n",
    "* cost = - Y * log(Y-cap)  \n",
    "  \n",
    "### Accuracy function  \n",
    "* Accuracy = (correct)/(correct + incorrect) %  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "K = 4\n",
    "L = 8\n",
    "M = 12\n",
    "\n",
    "W1 = tf.Variable(tf.truncated_normal([5, 5, 1, K], stddev=0.5), name='w1')\n",
    "b1 = tf.Variable(tf.truncated_normal([K], stddev=0.5), name='b1')\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.5), name='w2')\n",
    "b2 = tf.Variable(tf.truncated_normal([L], stddev=0.5), name='b2')\n",
    "\n",
    "W3 = tf.Variable(tf.truncated_normal([4, 4, L, M], stddev=0.5), name='w3')\n",
    "b3 = tf.Variable(tf.truncated_normal([M], stddev=0.5), name='b3')\n",
    "\n",
    "N = 200\n",
    "\n",
    "W4 = tf.Variable(tf.truncated_normal([7*7*M, N], stddev=0.5), name='w4')\n",
    "b4 = tf.Variable(tf.truncated_normal([N], stddev=0.5), name='b4')\n",
    "\n",
    "W5 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.5), name='w5')\n",
    "b5 = tf.Variable(tf.truncated_normal([10], stddev=0.5), name='b5')\n",
    "\n",
    "# summaries of weights\n",
    "if True:\n",
    "    tf.summary.histogram(\"weights/w1\", W1)\n",
    "    tf.summary.histogram(\"biases/b1\", b1)\n",
    "    tf.summary.histogram(\"weights/w2\", W2)\n",
    "    tf.summary.histogram(\"biases/b2\", b2)\n",
    "    tf.summary.histogram(\"weights/w3\", W3)\n",
    "    tf.summary.histogram(\"biases/b3\", b3)\n",
    "    tf.summary.histogram(\"weights/w4\", W4)\n",
    "    tf.summary.histogram(\"biases/b4\", b4)\n",
    "    tf.summary.histogram(\"weights/w5\", W5)\n",
    "    tf.summary.histogram(\"biases/b5\", b5)\n",
    "\n",
    "# convolution-layers\n",
    "Y1 = tf.nn.relu(tf.nn.conv2d(X, W1, strides=[1,1,1,1], padding='SAME') + b1, name='conv_layer_1')\n",
    "Y2 = tf.nn.sigmoid(tf.nn.conv2d(Y1, W2, strides=[1,2,2,1], padding='SAME') + b2, name='conv_layer_2')\n",
    "Y3 = tf.nn.sigmoid(tf.nn.conv2d(Y2, W3, strides=[1,2,2,1], padding='SAME') + b3, name='conv_layer_3')\n",
    "\n",
    "# fully-connected-layer\n",
    "YY = tf.reshape(Y3, shape=[-1, 7*7*M], name='fcc')\n",
    "YY = tf.nn.dropout(YY, keep_prob)\n",
    "\n",
    "# hidden-layer\n",
    "Y4 = tf.nn.sigmoid(tf.matmul(YY, W4) + b4, name='hidden_layer')\n",
    "Y4 = tf.nn.dropout(Y4, keep_prob)\n",
    "\n",
    "# model-output\n",
    "Y = tf.nn.softmax(tf.matmul(Y4, W5) + b5, name='ouput_layer')\n",
    "\n",
    "# add weights to collection\n",
    "tf.add_to_collection('conv_weights', W1)\n",
    "tf.add_to_collection('conv_weights', W2)\n",
    "tf.add_to_collection('conv_weights', W3)\n",
    "\n",
    "# add output to collection\n",
    "tf.add_to_collection('conv_output', Y1)\n",
    "tf.add_to_collection('conv_output', Y2)\n",
    "tf.add_to_collection('conv_output', Y3)\n",
    "\n",
    "# cost\n",
    "cross_entropy = -tf.reduce_sum(Y_ * tf.log(Y), name='cross_entropy')\n",
    "tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
    "\n",
    "# accuracy\n",
    "is_correct = tf.equal(tf.argmax(Y, 1), tf.argmax(Y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32), name='accuracy')\n",
    "tf.summary.scalar(\"accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(cross_entropy)\n",
    "\n",
    "# summaries of h_params\n",
    "tf.summary.scalar(\"number_of_epochs\", n_epochs)\n",
    "tf.summary.scalar(\"mini_batch_size\", batch_size)\n",
    "tf.summary.scalar(\"learning_rate\", optimizer._lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "* Mini-batch method used\n",
    "* train & test results stored separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "sess = tf.InteractiveSession()\n",
    "# variable-initializer\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "summaries = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(LOG_DIR + \"train\", graph=tf.get_default_graph())\n",
    "test_writer = tf.summary.FileWriter(LOG_DIR + \"test\")\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # feed train-data\n",
    "    batch_X, batch_Y = mnist.train.next_batch(batch_size)\n",
    "    train_data = {X: batch_X, \n",
    "                  Y_: batch_Y,\n",
    "                  learning_rate: 0.003,\n",
    "                  keep_prob: 0.7\n",
    "                 }\n",
    "    \n",
    "    summ, _ = sess.run([summaries, train_step], feed_dict=train_data)\n",
    "    train_writer.add_summary(summ, global_step=i)\n",
    "    \n",
    "    # feed test-data\n",
    "    test_data = {X: mnist.test.images, \n",
    "                 Y_: mnist.test.labels,\n",
    "                 learning_rate: 0.003,\n",
    "                 keep_prob: 1\n",
    "                }\n",
    "    \n",
    "    summ = sess.run(summaries, feed_dict=test_data)\n",
    "    test_writer.add_summary(summ, global_step=i)\n",
    "    \n",
    "    # print\n",
    "    if i%4 == 0:\n",
    "        print (\"Iteration: \", i, end='\\r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize weights of all convolutional layers\n",
    "# no need for feed dictionary here\n",
    "conv_weights = sess.run([tf.get_collection('conv_weights')])\n",
    "for i, c in enumerate(conv_weights[0]):\n",
    "    utility.plot_conv_weights(c, 'conv{}'.format(i))\n",
    "\n",
    "print ('Weights have been visualized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get output of all convolutional layers\n",
    "# here we need to provide a random input image\n",
    "conv_out = sess.run([tf.get_collection('conv_output')], feed_dict={X: mnist.test.images[:1]})\n",
    "for i, c in enumerate(conv_out[0]):\n",
    "    utility.plot_conv_output(c, 'conv{}'.format(i))\n",
    "    \n",
    "print ('Activations have been visualized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
